{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Regularization of Linear Regression\n",
    "\n",
    "본 코드는 Ridge Regularization의 구현 예시입니다.\n",
    "본 코드는 0과 1 사이의 X 들을 싸인함수에 fitting할때, Regularization이 Regression 에 미치는 영향을 살펴봅니다.\n",
    "\n",
    "## 실제 구현부\n",
    "\n",
    "Regularization은 학습의 정확도는 조금 포기하면서, 일반적인 데이터를 충분히 잘 설명할 수 있도록 모델을 데이터에 대해 다소 둔감하게 만드는 기술입니다. 즉, 기존의 Loss Function 에 Regularization term을 더한 Loss Function을 최소화하는 매개변수를 찾습니다. 그 중 Ridge Regularization을 이용한 Loss Function은 다음과 같습니다.\n",
    "$$E(w)=\\frac{1}{2}\\sum_{n=0}^N (train_n-g(x_n,w))^2+\\frac{\\lambda}{2}\\lVert w \\rVert ^2$$\n",
    "\n",
    "위 Loss Function을 최소화하고자 미분계수가 0이되는 지점을 찾아 w에 대해 정리하면 다음과 같은 closed form의 w를 찾을 수 있습니다.\n",
    "$$w = (X^TX+\\lambda I)^{-1}X^T*train$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.linalg import inv\n",
    "\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (2,2) and (100,) not aligned: 2 (dim 1) != 100 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-2641ab937ad4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mg2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolyval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m \u001b[1;31m# g2 = w'x\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mranda\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meye\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mg3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolyval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (2,2) and (100,) not aligned: 2 (dim 1) != 100 (dim 0)"
     ]
    }
   ],
   "source": [
    "x = np.arange(0,1,0.01)\n",
    "f = np.sin(2*np.pi*x)\n",
    "g1 = 0 * x; #g1=0으로 채워진 리스트\n",
    "\n",
    "w = np.polyfit(x,f,1); #linear regression 의 coefficient 값을 return\n",
    "g2 = np.polyval(w,x); # g2 = w'x\n",
    "\n",
    "plt.figure(1, figsize=(7,5)); \n",
    "plt.plot(x,f, color=\"blue\"); #sin함수 plotting\n",
    "plt.plot(x,g1, color=\"red\"); #y=0 상수함수 plotting\n",
    "plt.plot(x,g2, color=\"green\"); #y=w'x 선형회귀 plotting\n",
    "\n",
    "e1 = np.mean(np.dot((g1-f),(g1-f))); #상수함수의 RSS\n",
    "e2 = np.mean(np.dot((g2-f),(g2-f))); #선형회귀의 RSS\n",
    "\n",
    "# number of observations\n",
    "D = 10;\n",
    "trainingCaseG1 = np.zeros((D, 100));\n",
    "trainingCaseG2 = np.zeros((D, 100));\n",
    "trainingCaseG3 = np.zeros((D, 100));\n",
    "\n",
    "for i in range(D):\n",
    "    trainingCaseX = np.random.uniform(0,1,2); #0과 1사이의 수 2개를 무작위로 뽑음\n",
    "    trainingCaseF = np.sin(2*np.pi*trainingCaseX); #위 무작위 x에 대한 sin함수값\n",
    "    plt.plot(trainingCaseX,trainingCaseF, 'mo'); \n",
    "    \n",
    "    tempTrainingCaseX = np.zeros((2,2));\n",
    "    tempTrainingCaseX[0,:] = trainingCaseX;\n",
    "    tempTrainingCaseX[1,:] = [1,1];\n",
    "    randa = 1;\n",
    "    \n",
    "    # w = (X'*X+ramda*I)^(-1)*X'*trainF, linear regression 에 L2 정규화를 적용한 계수를 찾는 계산\n",
    "    w_r = np.dot(np.transpose(np.dot(inv(np.dot(np.transpose(tempTrainingCaseX),tempTrainingCaseX) + randa + np.eye(2)), np.transpose(tempTrainingCaseX))), np.transpose(trainingCaseF));\n",
    "    tempX = np.zeros((2,np.size(x))); #0으로 채워진 2X100 matrix\n",
    "    tempX[0,:] = x;\n",
    "    \n",
    "    for j in range(np.size(x)): #생성한 x의 갯수만큼 반복\n",
    "    tempX[1,j] = 1;\n",
    "    # regularized regression line, 1X100 Matrix\n",
    "    trainingCaseG3[i,:] = np.transpose(np.dot(np.transpose(w_r),tempX));  #y=w'x 하려면 x[0]=1, x[1] =x여야하지않을까?\n",
    "    \n",
    "    # First Degree Line\n",
    "    w = np.polyfit(trainingCaseX, trainingCaseF, 1);\n",
    "    trainingCaseG2[i,:] = np.polyval(w,x);\n",
    "\n",
    "    # Zero Degree Line\n",
    "    w = np.polyfit(trainingCaseX, trainingCaseF, 0);\n",
    "    trainingCaseG1[i,:] = np.polyval(w,x);\n",
    "    \n",
    "meanG1 = np.zeros((np.size(x),1));\n",
    "meanG2 = np.zeros((np.size(x),1));\n",
    "meanG3 = np.zeros((np.size(x),1));\n",
    "\n",
    "stdG1 = np.zeros((np.size(x),1));\n",
    "stdG2 = np.zeros((np.size(x),1));\n",
    "stdG3 = np.zeros((np.size(x),1));\n",
    "\n",
    "biasXG1 = np.zeros((np.size(x),1));\n",
    "biasXG2 = np.zeros((np.size(x),1));\n",
    "biasXG3 = np.zeros((np.size(x),1));\n",
    "    \n",
    "for j in range(np.size(x)):\n",
    "    meanG1[j] = np.mean(trainingCaseG1[:,j]);\n",
    "    stdG1[j] = np.std(trainingCaseG1[:,j]);\n",
    "    biasXG1[j] = (meanG1[j] - f[j])*(meanG1[j] - f[j]);\n",
    "    \n",
    "    meanG2[j] = np.mean(trainingCaseG2[:,j]);\n",
    "    stdG2[j] = np.std(trainingCaseG2[:,j]);\n",
    "    biasXG2[j] = (meanG2[j] - f[j])*(meanG2[j] - f[j]);\n",
    "    \n",
    "    meanG3[j] = np.mean(trainingCaseG3[:,j]);\n",
    "    stdG3[j] = np.std(trainingCaseG3[:,j]);\n",
    "    biasXG3[j] = (meanG3[j] - f[j])*(meanG3[j] - f[j]);\n",
    "    \n",
    "plt.errorbar(x,meanG3, yerr = stdG3);\n",
    "\n",
    "deviationG1 = np.zeros((np.size(x), D));\n",
    "deviationG2 = np.zeros((np.size(x), D));\n",
    "deviationG3 = np.zeros((np.size(x), D));\n",
    "\n",
    "for i in range(np.size(x)):\n",
    "    for j in range(D):\n",
    "        deviationG1[i,j] = (trainingCaseG1[j,i] - meanG1[i]) * (trainingCaseG1[j,i] - meanG1[i]); \n",
    "        deviationG2[i,j] = (trainingCaseG2[j,i] - meanG2[i]) * (trainingCaseG2[j,i] - meanG2[i]); \n",
    "        deviationG3[i,j] = (trainingCaseG3[j,i] - meanG3[i]) * (trainingCaseG3[j,i] - meanG3[i]); \n",
    "        \n",
    "biasG1 = np.mean(biasXG1); # y=0 의 bias\n",
    "biasG2 = np.mean(biasXG2); # linear regression 의 bias\n",
    "biasG2 = np.mean(biasXG3); # regularized linear regression 의 bias\n",
    "\n",
    "varG1 = np.mean(np.mean(deviationG1)); #  상수함수의 standard deviation\n",
    "varG2 = np.mean(np.mean(deviationG2)); # linear regression 의 standard deviation\n",
    "varG3 = np.mean(np.mean(deviationG3)); # regularized linear regression 의 standard deviation\n",
    "\n",
    "plt.show();\n",
    "print(\"bias of constant function is\", biasG1)\n",
    "print(\"bias of linear regression is\", biasG2)\n",
    "print(\"bias of regularized linear regression is\", biasG3)\n",
    "print(\"Variance of constant function is\", varG1)\n",
    "print(\"Variance of linear regression is\", varG2)\n",
    "print(\"Variance of regularized linear regression is\", varG3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
